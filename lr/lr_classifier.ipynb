{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First ICU Prediction using Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Inits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T15:41:08.128697Z",
     "start_time": "2019-06-07T15:41:08.116161Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T15:41:09.491925Z",
     "start_time": "2019-06-07T15:41:08.130240Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': PosixPath('../data'),\n",
       " 'workdir': PosixPath('../data/work_dir/lr'),\n",
       " 'dataset_csv': PosixPath('../data/processed_dataset.csv'),\n",
       " 'cols': ['class_label', 'scispacy_note'],\n",
       " 'modeldir': PosixPath('../data/work_dir/lr/models'),\n",
       " 'min_freq': 3,\n",
       " 'bc_threshold': 0.47,\n",
       " 'start_seed': 127}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy import interp, stats\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n",
    "\n",
    "from utils.splits import set_two_splits\n",
    "from utils.metrics import BinaryAvgMetrics\n",
    "from utils.plots import *\n",
    "from args import args\n",
    "vars(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T13:14:44.555160Z",
     "start_time": "2019-06-07T13:14:44.536305Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def top_words(vectorizer, clf, N):\n",
    "  feature_names = vectorizer.get_feature_names()\n",
    "  coefs_with_fns = sorted(zip(np.round(clf.coef_[0], 2), feature_names), reverse=True)\n",
    "  negative = coefs_with_fns[:N]\n",
    "  positive = coefs_with_fns[:-(N + 1):-1]\n",
    "\n",
    "  print(\"Words associated with 'positive': \")\n",
    "  for feat in positive:\n",
    "      print(feat)\n",
    "\n",
    "  print(\"Words associated with 'negative': \")\n",
    "  for feat in negative:\n",
    "      print(feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## LR Model Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T13:12:28.439026Z",
     "start_time": "2019-06-07T13:12:20.890648Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "ori_df = pd.read_csv(args.dataset_csv, usecols=args.cols)\n",
    "df = set_two_splits(ori_df.copy(), 'test', seed=seed)\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=args.min_freq, analyzer=str.split, sublinear_tf=True,\\\n",
    "                              ngram_range=(2,2))\n",
    "x_train = vectorizer.fit_transform(df.loc[(df['split'] == 'train')]['scispacy_note'])\n",
    "x_test = vectorizer.transform(df.loc[(df['split'] == 'test')]['scispacy_note'])\n",
    "y_train = df.loc[(df['split'] == 'train')]['class_label'].to_numpy()\n",
    "y_test = df.loc[(df['split'] == 'test')]['class_label'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T13:12:30.930485Z",
     "start_time": "2019-06-07T13:12:29.923907Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression(dual=True, class_weight='balanced', solver='liblinear', C=1.5)\n",
    "clf.fit(x_train, y_train)\n",
    "prob = clf.predict_proba(x_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T13:14:17.385469Z",
     "start_time": "2019-06-07T13:14:16.871647Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "plot_thresh_range(ax, y_test, prob, 0.3, 0.7, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T13:14:32.246668Z",
     "start_time": "2019-06-07T13:14:32.112804Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_pred = (prob > 0.47).astype(np.int64)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn,fp,fn,tp = cm[0][0],cm[0][1],cm[1][0],cm[1][1]\n",
    "prevalence = (fn+tp)/(tn+fp+fn+tp)\n",
    "sensitivity = tp/(tp+fn)\n",
    "specificity = tn/(tn+fp)\n",
    "ppv = tp/(tp+fp)\n",
    "npv = tn/(tn+fn)\n",
    "f1 = (2*ppv*sensitivity)/(ppv+sensitivity)\n",
    "auroc = roc_auc_score(y_test, prob)\n",
    "\n",
    "d = {\n",
    "  'sensitivity': np.round(sensitivity, 3),\n",
    "  'specificity': np.round(specificity, 3),\n",
    "  'ppv': np.round(ppv, 3),\n",
    "  'npv': np.round(npv, 3),\n",
    "  'f1': np.round(f1, 3),\n",
    "  'auroc': np.round(auroc, 3),\n",
    "  'prevalence': np.round(prevalence, 3),  \n",
    "}\n",
    "\n",
    "metrics = pd.DataFrame(d.values(), index=d.keys(), columns=['Value'])\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T13:14:36.508824Z",
     "start_time": "2019-06-07T13:14:36.272179Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(16, 4))\n",
    "\n",
    "plot_confusion_matrix(ax[0], cm, classes=['not imminent', 'imminent'], normalize=False, title='Confusion matrix')\n",
    "plot_confusion_matrix(ax[1], cm, classes=['not imminent', 'imminent'], normalize=True,\\\n",
    "                      title='Normalized confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T13:14:49.422566Z",
     "start_time": "2019-06-07T13:14:49.312719Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# clf = pickle.load(open(args.modeldir/'sklearn-lr.pkl', 'rb'))\n",
    "top_words(vectorizer, clf, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T13:30:29.267656Z",
     "start_time": "2019-06-07T13:30:12.377848Z"
    }
   },
   "outputs": [],
   "source": [
    "ori_df = pd.read_csv(args.dataset_csv, usecols=args.cols)\n",
    "preds = []\n",
    "targs = []\n",
    "probs = []\n",
    "\n",
    "for seed in range(42, 44):\n",
    "  if seed % 10 == 0:\n",
    "    print(f\"Running classifier with seed {seed}\")\n",
    "  df = set_two_splits(ori_df.copy(), 'test', seed=seed)\n",
    "  vectorizer = TfidfVectorizer(min_df=args.min_freq, analyzer=str.split, ngram_range=(2,2))\n",
    "  \n",
    "  x_train = vectorizer.fit_transform(df.loc[(df['split'] == 'train')]['scispacy_note'])\n",
    "  x_test = vectorizer.transform(df.loc[(df['split'] == 'test')]['scispacy_note'])\n",
    "  \n",
    "  y_train = df.loc[(df['split'] == 'train')]['class_label'].to_numpy()\n",
    "  y_test = df.loc[(df['split'] == 'test')]['class_label'].to_numpy()\n",
    "  targs.append(y_test)\n",
    "  \n",
    "  clf = LogisticRegression(dual=True, class_weight='balanced', solver='liblinear', C=1.5)\n",
    "  clf.fit(x_train, y_train)  \n",
    "  pickle.dump(clf, open(args.modeldir/f'lr_seed_{seed}.pkl', 'wb'))\n",
    "  \n",
    "  pos_prob = clf.predict_proba(x_test)[:, 1]\n",
    "  probs.append(pos_prob)\n",
    "  \n",
    "  y_pred = (pos_prob > args.bc_threshold).astype(np.int64)\n",
    "  preds.append(y_pred)\n",
    "\n",
    "with open(args.workdir/f'preds.pkl', 'wb') as f:\n",
    "  pickle.dump(targs, f)\n",
    "  pickle.dump(preds, f)\n",
    "  pickle.dump(probs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T13:34:18.639631Z",
     "start_time": "2019-06-07T13:34:10.987732Z"
    }
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "ori_df = pd.read_csv(args.dataset_csv, usecols=args.cols)\n",
    "df = set_two_splits(ori_df.copy(), 'test', seed=seed)\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=args.min_freq, analyzer=str.split, sublinear_tf=True,\\\n",
    "                              ngram_range=(2,2))\n",
    "x_train = vectorizer.fit_transform(df.loc[(df['split'] == 'train')]['scispacy_note'])\n",
    "x_test = vectorizer.transform(df.loc[(df['split'] == 'test')]['scispacy_note'])\n",
    "y_train = df.loc[(df['split'] == 'train')]['class_label'].to_numpy()\n",
    "y_test = df.loc[(df['split'] == 'test')]['class_label'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T13:34:18.667800Z",
     "start_time": "2019-06-07T13:34:18.641849Z"
    }
   },
   "outputs": [],
   "source": [
    "fnames = [f'lr_seed_{seed}.pkl' for seed in range(42, 44)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T13:34:18.690671Z",
     "start_time": "2019-06-07T13:34:18.669398Z"
    }
   },
   "outputs": [],
   "source": [
    "clf = pickle.load(open(args.modeldir/fnames[0], 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T13:34:27.960658Z",
     "start_time": "2019-06-07T13:34:27.939853Z"
    }
   },
   "outputs": [],
   "source": [
    "pos_prob = clf.predict_proba(x_test)[:, 1]\n",
    "y_pred = (pos_prob > args.bc_threshold).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T13:35:17.205291Z",
     "start_time": "2019-06-07T13:35:16.876435Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = (prob > args.bc_threshold).astype(np.int64)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn,fp,fn,tp = cm[0][0],cm[0][1],cm[1][0],cm[1][1]\n",
    "prevalence = (fn+tp)/(tn+fp+fn+tp)\n",
    "sensitivity = tp/(tp+fn)\n",
    "specificity = tn/(tn+fp)\n",
    "ppv = tp/(tp+fp)\n",
    "npv = tn/(tn+fn)\n",
    "f1 = (2*ppv*sensitivity)/(ppv+sensitivity)\n",
    "auroc = roc_auc_score(y_test, prob)\n",
    "\n",
    "d = {\n",
    "  'sensitivity': np.round(sensitivity, 3),\n",
    "  'specificity': np.round(specificity, 3),\n",
    "  'ppv': np.round(ppv, 3),\n",
    "  'npv': np.round(npv, 3),\n",
    "  'f1': np.round(f1, 3),\n",
    "  'auroc': np.round(auroc, 3),\n",
    "  'prevalence': np.round(prevalence, 3),  \n",
    "}\n",
    "\n",
    "metrics = pd.DataFrame(d.values(), index=d.keys(), columns=['Value'])\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taken from [here](https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/):\n",
    "\n",
    "1. Prevalence: `(fn + tp) / total`\n",
    "2. Sensitivity: AKA recall, true positive rate `tp / (tp + fn)`\n",
    "3. Specificity: AKA true negative rate `tn / (tn + fp)`\n",
    "4. Positive Predictive Value (PPV): AKA precision `tp / (tp + fp)`\n",
    "5. Negative Predictive Value (NPV): `tn / (tn + fn)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T15:42:08.671175Z",
     "start_time": "2019-06-07T15:42:08.550282Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(args.workdir/f'preds.pkl', 'rb') as f:\n",
    "  targs = pickle.load(f)\n",
    "  preds = pickle.load(f)\n",
    "  probs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T15:42:10.074278Z",
     "start_time": "2019-06-07T15:42:10.003098Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Number of Runs: 2\n",
       "Average Prevalence of positive class: 0.235"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bam = BinaryAvgMetrics(targs, preds, probs)\n",
    "bam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T11:31:25.263974Z",
     "start_time": "2019-05-23T11:31:25.064877Z"
    }
   },
   "outputs": [],
   "source": [
    "bam.get_avg_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T11:32:19.090158Z",
     "start_time": "2019-05-23T11:32:19.052429Z"
    }
   },
   "outputs": [],
   "source": [
    "bam.get_avg_metrics(conf=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T11:03:05.126982Z",
     "start_time": "2019-05-23T11:03:04.434470Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "plot_mean_roc(ax, bam.targs, bam.probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T11:03:09.226235Z",
     "start_time": "2019-05-23T11:03:08.971012Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "plot_confusion_matrix(ax[0], bam.cm_avg, classes=['not imminent', 'imminent'], normalize=False,\\\n",
    "                      title='Confusion Matrix Over Runs')\n",
    "plot_confusion_matrix(ax[1], bam.cm_avg, classes=['not imminent', 'imminent'], normalize=True,\\\n",
    "                      title='Normalized Confusion Matrix Over Runs')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
