{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First ICU Prediction using CNN with Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Inits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T10:02:31.057983Z",
     "start_time": "2019-06-10T10:02:31.021087Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T10:02:35.103755Z",
     "start_time": "2019-06-10T10:02:31.070364Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'workdir': PosixPath('../data/work_dir/cnn'),\n",
       " 'dataset_csv': PosixPath('../data/processed_dataset.csv'),\n",
       " 'batch_size': 128,\n",
       " 'min_freq': 3,\n",
       " 'hidden_dim': 100,\n",
       " 'dropout_p': 0.1,\n",
       " 'emb_dropout': 0.1,\n",
       " 'n_channels': 100,\n",
       " 'lr': 0.001,\n",
       " 'wd': 0.0,\n",
       " 'n_epochs': 15,\n",
       " 'checkpointer_save_total': 1,\n",
       " 'emb_path': PosixPath('../pretrained/glove/glove.6B.50d.txt'),\n",
       " 'emb_sz': 50,\n",
       " 'checkpointer_prefix': 'glove50_cnn',\n",
       " 'device': 'cuda:2',\n",
       " 'checkpointer_name': 'epoch',\n",
       " 'checkpointer_save_every': 5,\n",
       " 'early_stop_patience': 10,\n",
       " 'bc_threshold': 0.23,\n",
       " 'cols': ['class_label', 'scispacy_note'],\n",
       " 'start_seed': 127}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import OrderedDict\n",
    "from functools import partial\n",
    "from sklearn.metrics import *\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from ignite.engine import Events, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss, Precision, Recall\n",
    "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
    "\n",
    "from cnn_classifier.dataset import NoteDataset\n",
    "from cnn_classifier.model import NoteClassifier\n",
    "from cnn_classifier.containers import ModelContainer, DataContainer\n",
    "from cnn_classifier.trainer import IgniteTrainer\n",
    "from utils.embeddings import PretrainedEmbeddings\n",
    "from utils.plots import *\n",
    "from utils.metrics import BinaryAvgMetrics, get_best_model\n",
    "\n",
    "from utils.splits import *\n",
    "from args import args\n",
    "vars(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T09:56:26.292633Z",
     "start_time": "2019-06-10T09:56:26.122152Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_sample(df, sample_pct=0.01, with_val=True, seed=None):\n",
    "  train = df.loc[(df['split']) == 'train'].sample(frac=sample_pct, random_state=seed)\n",
    "  train.reset_index(inplace=True, drop=True)\n",
    "\n",
    "  if with_val:\n",
    "    val = df.loc[(df['split']) == 'val'].sample(frac=sample_pct, random_state=seed)\n",
    "    val.reset_index(inplace=True, drop=True)\n",
    "    return pd.concat([train, val], axis=0) \n",
    "\n",
    "  return train\n",
    "\n",
    "def convert_probs(output, thresh):\n",
    "  y_pred, y = output\n",
    "  y_pred = (torch.sigmoid(y_pred) > thresh).long()\n",
    "  return y_pred, y\n",
    "\n",
    "def predict_proba(clf, x_test):\n",
    "  return torch.sigmoid(clf(x_test)).detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Sample Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T20:59:12.652510Z",
     "start_time": "2019-06-07T20:59:09.491097Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "ori_df = pd.read_csv(args.dataset_csv, usecols=args.cols)\n",
    "df = set_all_splits(ori_df.copy(), 0.1, 0.1, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-27T19:20:06.822131Z",
     "start_time": "2019-05-27T19:20:06.793234Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sample_df = get_sample(df)\n",
    "sample_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T22:43:22.390777Z",
     "start_time": "2019-06-07T22:42:52.146242Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dc = DataContainer(df, NoteDataset, args.workdir, bs=args.batch_size, with_test=True,\\\n",
    "                   min_freq=args.min_freq, create_vec=True, weighted_sampling=True)\n",
    "itr = iter(dc.train_dl)\n",
    "\n",
    "pe = PretrainedEmbeddings.from_file(args.emb_path)\n",
    "pe.make_custom_embeddings(dc.get_vocab_tokens())\n",
    "\n",
    "classifier = NoteClassifier(args.emb_sz, dc.get_vocab_size(), args.n_channels, args.hidden_dim, dc.n_classes,\\\n",
    "                            dropout_p=args.dropout_p, emb_dropout=args.emb_dropout,\\\n",
    "                            pretrained=pe.custom_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T21:01:41.603592Z",
     "start_time": "2019-06-07T21:01:41.408897Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(classifier.parameters(), lr=args.lr, weight_decay=args.wd)\n",
    "reduce_lr = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', 0.5, 1)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "mc = ModelContainer(classifier, loss_fn, optimizer, reduce_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T21:01:49.763038Z",
     "start_time": "2019-06-07T21:01:42.665298Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x, y = next(itr)\n",
    "y_pred = classifier(x)\n",
    "print(loss_fn(y_pred, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T21:54:39.646698Z",
     "start_time": "2019-06-07T21:04:11.356868Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bce_logits_wrapper = partial(convert_probs, thresh=args.bc_threshold)\n",
    "metrics = OrderedDict({ 'loss': Loss(loss_fn)})\n",
    "ig = IgniteTrainer(mc, dc, args, metrics, log_training=True, early_stop=True)\n",
    "model_name = ig.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T22:43:34.723354Z",
     "start_time": "2019-06-07T22:43:28.651183Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x_test, targ = next(iter(dc.test_dl))\n",
    "x_test = x_test.to('cpu')\n",
    "targ = targ.to('cpu')\n",
    "classifier = classifier.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T22:52:34.346365Z",
     "start_time": "2019-06-07T22:50:59.064225Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "prob = predict_proba(classifier, x_test)\n",
    "pred = (prob > args.bc_threshold).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T22:50:01.904067Z",
     "start_time": "2019-06-07T22:49:59.521227Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "plot_thresh_range(ax, targ, prob, 0.1, 0.9, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T22:54:32.669358Z",
     "start_time": "2019-06-07T22:54:32.582690Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(targ, pred)\n",
    "tn,fp,fn,tp = cm[0][0],cm[0][1],cm[1][0],cm[1][1]\n",
    "prevalence = (fn+tp)/(tn+fp+fn+tp)\n",
    "sensitivity = tp/(tp+fn)\n",
    "specificity = tn/(tn+fp)\n",
    "ppv = tp/(tp+fp)\n",
    "npv = tn/(tn+fn)\n",
    "f1 = (2*ppv*sensitivity)/(ppv+sensitivity)\n",
    "auroc = roc_auc_score(targ, prob)\n",
    "\n",
    "d = {\n",
    "  'sensitivity': np.round(sensitivity, 3),\n",
    "  'specificity': np.round(specificity, 3),\n",
    "  'ppv': np.round(ppv, 3),\n",
    "  'npv': np.round(npv, 3),\n",
    "  'f1': np.round(f1, 3),\n",
    "  'auroc': np.round(auroc, 3),\n",
    "  'prevalence': np.round(prevalence, 3),  \n",
    "}\n",
    "\n",
    "metrics = pd.DataFrame(d.values(), index=d.keys(), columns=['Value'])\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T22:39:25.005551Z",
     "start_time": "2019-06-07T22:39:24.808259Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T23:02:08.861131Z",
     "start_time": "2019-06-07T23:02:08.438838Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(16, 4))\n",
    "\n",
    "plot_confusion_matrix(ax[0], cm, classes=['not imminent', 'imminent'], normalize=False, title='Confusion matrix')\n",
    "plot_confusion_matrix(ax[1], cm, classes=['not imminent', 'imminent'], normalize=True,\\\n",
    "                      title='Normalized confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-27T13:43:57.379625Z",
     "start_time": "2019-05-27T13:43:56.455972Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dc = DataContainer(df, NoteDataset, args.workdir, bs=args.batch_size, with_test=True,\\\n",
    "                   min_freq=args.min_freq, load_vec=True)\n",
    "\n",
    "print(dc.get_dataset_size())\n",
    "print(dc.get_batch_sizes())\n",
    "print(dc.get_num_batches())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-27T13:44:10.211949Z",
     "start_time": "2019-05-27T13:43:58.368212Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pe = PretrainedEmbeddings.from_file(args.emb_path)\n",
    "pe.make_custom_embeddings(dc.get_vocab_tokens())\n",
    "\n",
    "classifier = NoteClassifier(args.emb_sz, dc.get_vocab_size(), args.n_channels, args.hidden_dim, dc.n_classes,\\\n",
    "                            dropout_p=args.dropout_p, pretrained=pe.custom_embeddings)\n",
    "\n",
    "state_dict = torch.load(args.modelfile)\n",
    "classifier.load_state_dict(state_dict);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T10:42:51.919793Z",
     "start_time": "2019-05-20T10:42:51.730731Z"
    },
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T13:11:28.846243Z",
     "start_time": "2019-05-20T13:11:28.814144Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "log = pd.read_csv(args.workdir/'training_log.csv')\n",
    "log = log[:-1]\n",
    "log.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T13:11:29.328015Z",
     "start_time": "2019-05-20T13:11:28.847463Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15,10))\n",
    "log.plot(x='epoch', y=['training_loss', 'validation_loss'], kind='line',\n",
    "                      title='Training and validation loss', ax=axes[0][0])\n",
    "log.plot(x='epoch', y=['training_accuracy', 'validation_accuracy'], kind='line',\n",
    "                      title='Training and validation accuracy', ax=axes[0][1])\n",
    "log.plot(x='epoch', y=['training_precision', 'validation_precision'], kind='line',\n",
    "                      title='Training and validation precision', ax=axes[1][0])\n",
    "log.plot(x='epoch', y=['training_recall', 'validation_recall'], kind='line',\n",
    "                      title='Training and validation recall', ax=axes[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-27T11:28:05.383469Z",
     "start_time": "2019-05-27T11:28:05.305222Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "bce_logits_wrapper = partial(convert_probs, thresh=args.bc_threshold)\n",
    "metrics = OrderedDict({ 'loss': Loss(loss_fn), 'sensitivity': Recall(bce_logits_wrapper),\\\n",
    "                       'ppv': Precision(bce_logits_wrapper)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-27T11:29:03.702303Z",
     "start_time": "2019-05-27T11:28:08.616567Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "evaluator = create_supervised_evaluator(classifier, metrics=metrics)\n",
    "\n",
    "@evaluator.on(Events.COMPLETED)\n",
    "def log_testing_results(engine):\n",
    "  metrics = engine.state.metrics\n",
    "  for metric in metrics.keys():\n",
    "    print(f\"{metric} {metrics[metric]:0.3f}\")\n",
    "\n",
    "evaluator.run(dc.test_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-27T11:29:12.539520Z",
     "start_time": "2019-05-27T11:29:12.317032Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# sort weights\n",
    "emb = classifier.emb.weight.detach()[0]\n",
    "_, idxs = torch.sort(emb, dim=0, descending=True)\n",
    "idxs = idxs.numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-27T11:29:14.090888Z",
     "start_time": "2019-05-27T11:29:14.070124Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Top 20 words\n",
    "print(\"Influential words in positive class:\")\n",
    "print(\"--------------------------------------\")\n",
    "for i in range(20):\n",
    "    print(dc.vectorizer.vocab.lookup_idx(idxs[i]))\n",
    "    \n",
    "print(\"====\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-27T11:29:17.285476Z",
     "start_time": "2019-05-27T11:29:17.266591Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Top 20 words\n",
    "print(\"Influential words in negative class:\")\n",
    "print(\"--------------------------------------\")\n",
    "idxs.reverse()\n",
    "for i in range(20):\n",
    "    print(dc.vectorizer.vocab.lookup_idx(idxs[i]))\n",
    "    \n",
    "print(\"====\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T11:18:05.216079Z",
     "start_time": "2019-06-09T11:18:05.194207Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# run this if preds.pkl is not generated\n",
    "targs,preds,probs = [],[],[]\n",
    "\n",
    "for i in range(4):\n",
    "  with open(args.workdir/f'preds_{i+1}.pkl', 'rb') as f:\n",
    "    targs_i = pickle.load(f)\n",
    "    preds_i = pickle.load(f)\n",
    "    probs_i = pickle.load(f)\n",
    "    \n",
    "  targs += targs_i\n",
    "  preds += preds_i\n",
    "  probs += probs_i\n",
    "\n",
    "with open(args.workdir/'preds.pkl', 'wb') as f:\n",
    "  pickle.dump(targs, f)\n",
    "  pickle.dump(preds, f)\n",
    "  pickle.dump(probs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Taken from [here](https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/):\n",
    "\n",
    "1. Prevalence: `(fn + tp) / total`\n",
    "2. Sensitivity: AKA recall, true positive rate `tp / (tp + fn)`\n",
    "3. Specificity: AKA true negative rate `tn / (tn + fp)`\n",
    "4. Positive Predictive Value (PPV): AKA precision `tp / (tp + fp)`\n",
    "5. Negative Predictive Value (NPV): `tn / (tn + fn)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T11:22:38.228243Z",
     "start_time": "2019-06-09T11:22:38.186606Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open(args.workdir/'preds.pkl', 'rb') as f:\n",
    "  targs = pickle.load(f)\n",
    "  preds = pickle.load(f)\n",
    "  probs = pickle.load(f)\n",
    "  \n",
    "fnames = [f'glove50_cnn_{seed}_epoch_15.pth' for seed in range(args.start_seed, args.start_seed + 100)]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T11:22:39.857771Z",
     "start_time": "2019-06-09T11:22:38.229697Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bam = BinaryAvgMetrics(targs, preds, probs)\n",
    "bam.get_avg_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T11:22:40.078693Z",
     "start_time": "2019-06-09T11:22:39.859148Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "get_best_model(bam, fnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T11:22:50.928172Z",
     "start_time": "2019-06-09T11:22:50.749387Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bam.get_avg_metrics(conf=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T11:22:54.501853Z",
     "start_time": "2019-06-09T11:22:53.829235Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "plot_mean_roc(ax, bam.targs, bam.probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T11:22:54.761334Z",
     "start_time": "2019-06-09T11:22:54.503385Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "plot_confusion_matrix(ax[0], bam.cm_avg, classes=['not imminent', 'imminent'], normalize=False,\\\n",
    "                      title='Confusion Matrix Over Runs')\n",
    "plot_confusion_matrix(ax[1], bam.cm_avg, classes=['not imminent', 'imminent'], normalize=True,\\\n",
    "                      title='Normalized Confusion Matrix Over Runs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T10:03:42.441180Z",
     "start_time": "2019-06-10T10:02:35.108491Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:cnn_classifier.containers: No validation split provided. Using single sample of the training set.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(args.dataset_csv, usecols=args.cols)\n",
    "df['split'] = 'train'\n",
    "dc = DataContainer(df, NoteDataset, args.workdir, bs=args.batch_size, with_test=False,\\\n",
    "                   min_freq=args.min_freq, create_vec=True, weighted_sampling=True)\n",
    "\n",
    "pe = PretrainedEmbeddings.from_file(args.emb_path)\n",
    "pe.make_custom_embeddings(dc.get_vocab_tokens())\n",
    "\n",
    "classifier = NoteClassifier(args.emb_sz, dc.get_vocab_size(), args.n_channels, args.hidden_dim, dc.n_classes,\\\n",
    "                            dropout_p=args.dropout_p, emb_dropout=args.emb_dropout,\\\n",
    "                            pretrained=pe.custom_embeddings)\n",
    "\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.lr, weight_decay=args.wd)\n",
    "reduce_lr = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', 0.5, 1)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "mc = ModelContainer(classifier, loss_fn, optimizer, reduce_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T10:44:20.849926Z",
     "start_time": "2019-06-10T10:04:33.185899Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/15]: [397/397] 100%|██████████, loss=5.93e-01 [01:56<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss 0.581 \n",
      "Validation loss 0.486 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/15]: [397/397] 100%|██████████, loss=4.78e-01 [01:55<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss 0.486 \n",
      "Validation loss 0.000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/15]: [397/397] 100%|██████████, loss=3.89e-01 [01:55<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss 0.374 \n",
      "Validation loss 0.000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/15]: [397/397] 100%|██████████, loss=3.32e-01 [01:55<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss 0.304 \n",
      "Validation loss 0.000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/15]: [397/397] 100%|██████████, loss=2.80e-01 [01:55<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss 0.252 \n",
      "Validation loss 0.000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [6/15]: [397/397] 100%|██████████, loss=2.38e-01 [01:55<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss 0.236 \n",
      "Validation loss 0.000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [7/15]: [397/397] 100%|██████████, loss=2.10e-01 [01:55<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss 0.206 \n",
      "Validation loss 0.000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [8/15]: [397/397] 100%|██████████, loss=1.97e-01 [01:55<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss 0.174 \n",
      "Validation loss 0.000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [9/15]: [397/397] 100%|██████████, loss=1.89e-01 [01:55<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss 0.166 \n",
      "Validation loss 0.000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [10/15]: [397/397] 100%|██████████, loss=1.78e-01 [01:55<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss 0.159 \n",
      "Validation loss 0.000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [11/15]: [397/397] 100%|██████████, loss=1.77e-01 [01:55<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss 0.153 \n",
      "Validation loss 0.000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [12/15]: [397/397] 100%|██████████, loss=1.77e-01 [01:54<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss 0.150 \n",
      "Validation loss 0.000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [13/15]: [397/397] 100%|██████████, loss=1.70e-01 [01:55<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss 0.147 \n",
      "Validation loss 0.000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [14/15]: [397/397] 100%|██████████, loss=1.68e-01 [01:54<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss 0.149 \n",
      "Validation loss 0.000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [15/15]: [397/397] 100%|██████████, loss=1.68e-01 [01:54<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss 0.140 \n",
      "Validation loss 0.000 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'glove50_cnn_epoch_4.pth'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = OrderedDict({ 'loss': Loss(loss_fn)})\n",
    "ig = IgniteTrainer(mc, dc, args, metrics, log_training=False, early_stop=False)\n",
    "ig.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T11:05:36.381837Z",
     "start_time": "2019-06-10T11:05:36.109099Z"
    }
   },
   "outputs": [],
   "source": [
    "dc.train_ds.save_vectorizer(args.workdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T23:23:18.097254Z",
     "start_time": "2019-06-09T23:23:17.260760Z"
    }
   },
   "outputs": [],
   "source": [
    "test_ds = NoteDataset.load_data_and_vectorizer(df, dc.vectorizer)\n",
    "test_dl = DataLoader(test_ds, len(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T11:05:50.196396Z",
     "start_time": "2019-06-10T11:05:50.158871Z"
    }
   },
   "outputs": [],
   "source": [
    "state_dict = torch.load(args.workdir/'models/glove50_cnn_epoch_15.pth', map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T11:07:07.379354Z",
     "start_time": "2019-06-10T11:07:07.301102Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(args.workdir/'full_data_model.pkl', 'wb') as f:\n",
    "  pickle.dump(state_dict, f)\n",
    "  pickle.dump(dc.train_ds.vectorizer,f )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T23:23:57.650251Z",
     "start_time": "2019-06-09T23:23:57.626779Z"
    }
   },
   "outputs": [],
   "source": [
    "classifier.load_state_dict(state_dict);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T23:24:46.111151Z",
     "start_time": "2019-06-09T23:23:59.494789Z"
    }
   },
   "outputs": [],
   "source": [
    "x, y = next(iter(test_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T23:13:56.766051Z",
     "start_time": "2019-06-09T23:13:09.708861Z"
    }
   },
   "outputs": [],
   "source": [
    "x = x.to('cpu')\n",
    "classifier = classifier.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T23:14:09.439450Z",
     "start_time": "2019-06-09T23:14:09.265255Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-06-09T23:25:18.031Z"
    }
   },
   "outputs": [],
   "source": [
    "pred = classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sigmoid(clf(x_test)).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-06-09T23:05:05.058Z"
    }
   },
   "outputs": [],
   "source": [
    "prob = predict_proba(classifier, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
